\title { Data-Driven Classifiers }
\author{ Clint Ferrin     }
\date  { Fri Sep 28, 2017 }
\def\class { Neural Networks: ECE 5930 }
\documentclass{article}\makeatletter

\newcommand{\main} {
    % document setup
   \pageSetup
   \createTitlePage
   \includeHeader
   \createTableOfContents

   % content
   \overview
   \linear
   \quadratic
   \lqAnalysis
   \linLogisticRegression
   \kNearest
   \naiveBayes
   \optimalBayes
   \discussion
}

%  ┌────────────────────────┐
%  │     Extra Packages     │
%  └────────────────────────┘
    \usepackage[utf8]{inputenc}	% allows new character options
    \usepackage[a4paper]{geometry}   % Paper dimensions and margins
    \usepackage{fancyhdr}   % include document header
    \usepackage{amsmath}    % allows equations to be split
    \usepackage{bm}         % use of bold characters in math mode
    \usepackage{enumitem}   % create lists
    \usepackage{graphicx}	% manage images and graphics
    \usepackage{hyperref}	% creates hyper-link color options
    \usepackage{cleveref}	% (\Cref) include "Figure" on \reff 
    \usepackage{xparse}     % include high performing functions 
    \usepackage{xstring}    % StrSubstitute replace character
    \usepackage{floatrow}	% allows placement of figures [H]
    \usepackage{url}    	% package for url links
    \usepackage{titletoc}   % change Table of Contents settings
    \usepackage{caption}    % removes figure from LoF: \caption[]{}
    \usepackage{listings, lstautogobble} % includes ability to input code
    \usepackage{color}      % include colors for 
    \usepackage{courier}    % courier font for listings
    \usepackage{etoolbox}
    \usepackage{tabulary}	% columns size of their contents (on page)
    \usepackage{booktabs}   % allows for \toprule in tables

    \definecolor{mygreen}{RGB}{28,172,0}	% custom defined colors
    \definecolor{mylilas}{RGB}{170,55,241}
    \definecolor{mymauve}{rgb}{0.58,0,0.82}
    \definecolor{light-gray}{gray}{0.95} %the shade of grey that stack exchange uses

    \lstset {
        language=Python,
        backgroundcolor = \color{light-gray},
        breaklines		= true,
        keywordstyle    = \color{blue},
        morekeywords    = [2]{1}, keywordstyle=[2]{\color{black}},
        identifierstyle = \color{black},
        stringstyle     = \color{mylilas},
        commentstyle    = \color{mygreen},
        numbers         = left,
        numberstyle     = {\tiny \color{black}},	% size of the numbers
        numbersep       = 6pt, 						% distance of numbers from text
        emph            = [1]{as, for, end, break}, % bold for, end, break...
        emphstyle 		= [1]\color{red}, 			% emphasis
        basicstyle		= \footnotesize\ttfamily,	% set font to courier
        frameround      = ffff,                     % TR, BR, BL, TL. t(round)|f(flat)
        frame           = single,                   % single line all around
        showstringspaces= false,                    % blank spaces appear as written
        autogobble      = true
    }

%  ┌────────────────────────┐
%  │   General Functions    │
%  └────────────────────────┘
    % function to create magnitude bars around a function
    \newcommand{\norm}[1]{\left\lVert#1\right\rVert}

    \DeclareDocumentCommand{\reff}{m} {
        \edef\link{#1}
        \hspace{-0.5em}\hyperref[\link]{\Cref*{\link}} \hspace{-0.65em}
    }

    \DeclareDocumentCommand{\newFigure}{m m m m} {
        \edef\path{#1} \edef\figcaption{#2} \edef\size{#3}  

        % add size if not present
        \IfNoValueTF{#3} { % if 2 and 3 are NULL
            \def\size{0.75}
            }{}

            % add caption if not present
        \IfNoValueTF{#2} { % if 2 and 3 are NULL
            %\expandafter\StrSubstitute\expandafter{\temp}{-}{ }[\output]
            \newcommand\helphere{\temp}
            \StrBehind{\helphere}{/}[\figcaption]
        }{}

        \begin{figure}[H]
            \begin{center}
                \includegraphics[width=\size\textwidth]{\path}
                % I deleted the capitalize function because it wouldn't pass []
                % \capitalisewords{}
                \caption{\figcaption}
                \label{#4} % label gets rid of type and -.
            \end{center}
        \end{figure} 
    }

%  ┌────────────────────────┐
%  │   Content Functions    │
%  └────────────────────────┘
    \newcommand{\pageSetup} {

        \let\Title\@title
        \let\Date\@date
        \let\Author\@author

        % \patchcmd{\subsection}{\bfseries}{\normalsize}{}{}
        % \patchcmd{\subsection}{0.5em}{-0.5em}{}{}
        % \renewcommand{\thesubsection}{\normalsize\hspace{-1em}}

        % makes subsection appear in-line
        \renewcommand\subsection{\@startsection{subsubsection}{4}{\z@}%
                                     {-3.25ex\@plus -1ex \@minus -.2ex}%
                                     {-1.5ex \@plus -.2ex}% Formerly 1.5ex \@plus .2ex
                                     {\normalfont\normalsize\bfseries}}        \renewcommand{\thesubsection}{\hspace{-1em}}

                                     % turns off section numbers
                                     % \renewcommand{\thesection}{\hspace{-1em}}
                                     % \renewcommand{\partname}{}
                                     % \renewcommand{\thepart}{}

        \newgeometry{left=1in,bottom=1in,right=1in,top=1in} % page dims
        \setlength\parindent{0pt}	% set no tab on new paragraphs
        \parskip = \baselineskip	% set single skip after paragraphs
        \setlist{nosep,after=\vspace{\baselineskip}} % remove space on list
    }\hypersetup{				% hyper-links environment
        colorlinks,
            linkcolor	= {black},
            citecolor	= {black},
            urlcolor	= {black},
            pdfborderstyle={/S/U/W 1}
        }

    \newcommand{\createTitlePage} {
        \clearpage
        \begin{center}
            \thispagestyle{empty}
            \huge{\Title} 

            % line
            \vspace{0.25em}
            \line(1,0){250}\normalsize 

            \vspace{5mm}
            \class 

            \vspace{2.5cm}
                \begin{center}
                    \includegraphics[width=0.45\textwidth]{media/title.pdf}\par
                    Figure: Linear Regression Classifier
                \end{center}
            \vspace{2.5cm}

            \Author \vspace{-1em}

            Utah State University \vspace{-1em}

            \Date           \vspace{-1em}

            \pagenumbering{gobble} 
            \newpage
        \end{center}
    }

    \newcommand{\createTableOfContents} {
        \pagenumbering{roman}
        \clearpage
        % \newdimen\punktik
        % \def\cvak{\ifdim\punktik<6pt \global\punktik=3pt \else\global\punktik=3pt \fi}
        % \def\tocdots{\cvak\leaders\hbox to10pt{\kern\punktik.\hss}\hfill}
        % \titlecontents{section}[0em]{\vskip -1em}{}{\itshape}{\hfill\thecontentspage}
        % \titlecontents{subsection}[1em]{\vskip -1em}{}{} {\tocdots\thecontentspage}

        \tableofcontents 

        % \clearpage
        \renewcommand*\listfigurename{\normalsize{List of Figures}}
        \listoffigures

        \renewcommand*\listtablename{\normalsize{List of Tables}}
        \listoftables

        \newpage
        \pagenumbering{arabic}
    }

    \newcommand{\includeHeader} {
        \pagestyle{fancy}
        \fancyhf{}
        % \fancyhead[L]{Top Left}
        \fancyhead[L]{\Title}
        \fancyhead[R]{\nouppercase\leftmark}
        % \fancyhead[R]{Top Right}
        \renewcommand{\headrulewidth}{0.5pt}
        %\fancyfoot[L]{Bottom Left}
        \fancyfoot[C]{\thepage}
        %\fancyfoot[R]{Bottom Right}
        \renewcommand{\footrulewidth}{0.5pt}
    }

%  ┌────────────────────────┐
%  │    Written Content     │
%  └────────────────────────┘
    \DeclareDocumentCommand{\overview}{} {
        \section{Overview}
        This Document answers 17 questions that walk through pattern recognition on binary static data. To view the problem set and description of several of these classifiers, visit \href{https://drive.google.com/open?id=0B5NW7S3txe5UTE0xSHJHNWxJbEE}{\underline{this link}} or navigate to the following website:
        \texttt{https://drive.google.com/open?id=0B5NW7S3txe5UTE0xSHJHNWxJbEE}

        This document is not intended to be a comprehensive teaching document to describe each binary classifier, but rather aims to analyze some differences between a few classifiers as discussed in \reff{sec:discussion}.
    } 

    \DeclareDocumentCommand{\linear}{} {
        \section{Linear Regression}
        \subsection{Problem 1:} Show that the $\beta$ that minimizes $RSS(\beta)$ is
        $\beta  = (\bm{X}^T \bm{X})^{-1}\bm{X}^T\bm{y}$.


        To prove the previous statement, we will multiply the polynomial out, and find where the derivative equals zero in order to minimize $\beta$.


        \begin{equation*}\begin{aligned}
            (\bm{y} - \bm{X}\beta)^T (\bm{y} - \bm{X}\beta) = & 
            \bm{y}^T\bm{y} - \bm{y}^T \bm{X} \beta-\bm{X}^T\beta^T\bm{y} + \bm{X}^T \beta^T \bm{X}\beta\\
            = &\bm{X}^T \beta^T \bm{X}\beta - 2\bm{X}^T\beta^T\bm{y} + \bm{y}^T\bm{y}
        \end{aligned}\end{equation*}

        To find the minimized $\beta$ we will now take the derivative and solve for $\beta$ at zero.

        \begin{equation*}\begin{aligned}
            \frac{d}{d\beta}\bm{X}^T \beta^T \bm{X}\beta - 2 \bm{X}^T\beta^T\bm{y}
            + \bm{y}^T\bm{y} &=0\\
            2\bm{X}^T \bm{X} \beta - 2\bm{X}^T \bm{y} &= 0\\
            \bm{X}^T \bm{X} \beta &= \bm{X}^T\bm{y}\\
        \end{aligned}\end{equation*}
        \begin{equation}
            \beta  = (\bm{X}^T \bm{X})^{-1}\bm{X}^T\bm{y}
        \end{equation}

        \subsection{Problem 2:} Show that if the norm of $\norm{\bm{Y} - \bm{X\hat{B}}}^2$ is the Frobenius norm, then that the $\bm{\hat{B}}$ minimizing the same is determined by $ \bm{\hat{B}}  = (\bm{X}^T \bm{X})^{-1}\bm{X}^T\bm{Y}$ 

        Given that the Frobenius Norm for a matrix with real numbers is:
        \begin{equation*}
            \sqrt{Tr(\bm{AA}^T)} 
        \end{equation*}

        Then the Frobenius Norm of the problem statement is:
        \begin{equation*}\begin{aligned}
            \sqrt{Tr(\bm{Y}-\bm{X\hat{B}})(\bm{Y}-\bm{X\hat{B}})^T}&=
            \sqrt{Tr(\bm{X}^T \bm{\hat{B}}^T \bm{X}\bm{\hat{B}} - 2\bm{X}^T\bm{\hat{B}}^T\bm{Y} + \bm{Y}^T\bm{Y})} 
        \end{aligned}\end{equation*}

        To find the $\bm{\hat{B}}$ minimizing the problem statement, we will take the deriving with respect to $\bm{\hat{B}}$
        \begin{equation*}\begin{aligned}
            \frac{d}{d\bm{\hat{B}}} \norm{\sqrt{Tr(\bm{X}^T \bm{\hat{B}}^T \bm{X}\bm{\hat{B}} - 2\bm{X}^T\bm{\hat{B}}^T\bm{Y} + \bm{Y}^T\bm{Y})}}^2 &= 0\\
            2\bm{X}^T \bm{X} \bm{\hat{B}}- 2\bm{X}^T \bm{Y} &= 0\\
            \bm{X}^T \bm{X} \bm{\hat{B}} &= \bm{X}^T\bm{Y}\\
        \end{aligned}\end{equation*}

        \begin{equation}
            \bm{\hat{B}}  = (\bm{X}^T \bm{X})^{-1}\bm{X}^T\bm{Y}
        \end{equation}

        Note that the trace could be removed from the equation because the result of the trace was zero, meaning that the sum of the trace was filled with all zeros.

        \subsection{Problem 3:}
        Re-write the function \texttt{gendat2.m} into Python.
        Using the 100 points of training data in \texttt{classasgntrain1.dat}, write \texttt{PYTHON} code to train the coefficient matrix $\hat{\beta}$.

        The program produced the desired results and the outcome can be seen in \reff{fig:linear}. Note that the data is not completely linearly separable, and there were 29 errors on the wrong side of the line after it was drawn. The results of the outcome can be seen in \reff{tab:compare}.

        \newFigure{./media/linear.pdf}{Linear Classifier}{0.65}{fig:linear}
        \lstinputlisting[language=Python]{../python/3_linear.py}
    } 

    \DeclareDocumentCommand{\quadratic}{} {
        \section{Quadratic Regression} 

        \subsection{Problem 4:} For the data described in Problem 3, train the regression coefficient matrix $\hat{B}$. Determine the classification error rate on the training data and 10,000 points of test data (as before) and fill in the corresponding row of the results table. Plot the classification regions as before.

        The program performed as expected, and the outcome graph can be seen in \reff{fig:quad}. Note that due to the data that appears mostly linearly separable, the line does not curve much. The results of the program can be seen in \reff{tab:compare}.

        \newFigure{./media/quad.pdf}{Quadratic Regression Graph}{0.64}{fig:quad}
        \lstinputlisting[language=Python]{../python/4_quadratic.py}


        \subsection{Problem 5:} Show that $\text{log }P(\text{class} = k | X = x) =  \text{log}\hat{\pi}_k -\frac{1}{2}\text{log}|\hat{R}_k|-\frac{1}{2}(\bm{x}-\bm{\hat{\mu}}_k)^T\hat{R}_k^{-1}(\bm{x}-\hat{\bm{\mu}}_k)$ is true. In particular, make sure you understand what is meant by ``up to a constant which does not depend on the class"

          \begin{equation*}
              f_k(\bm{x}) = \frac{1}{(2\pi)^{d/2}|\hat{R}^{1/2}|}\text{exp}[-\frac{1}{2}(\bm{x}-\bm{\hat{\mu}}_k)^T\hat{R}_k^{-1}(\bm{x}-\hat{\bm{\mu}}_k)]
          \end{equation*}

        Using Bayes rule, we can produce the following form. Note: When using Bayes Rule, constants exuding the random variable can be eliminated without affecting the results:

        \begin{equation*}\begin{aligned}
            \hat{\pi}_k|\hat{R}_k|^{-1/2}\text{exp}[-\frac{1}{2}(\bm{x}-\bm{\hat{\mu}}_k)^T\hat{R}_k^{-1}(\bm{x}-\hat{\bm{\mu}}_k)] \\
        \end{aligned}\end{equation*}

        Now taking the log of the equation gives us:
        \begin{equation}
            \text{log}\hat{\pi}_k -\frac{1}{2}\text{log}|\hat{R}_k|-\frac{1}{2}(\bm{x}-\bm{\hat{\mu}}_k)^T\hat{R}_k^{-1}(\bm{x}-\hat{\bm{\mu}}_k)
        \end{equation}
    }


    \DeclareDocumentCommand{\lqAnalysis}{} {
        \section{Linear and Quadratic Discriminant Analysis}
        \subsection{Problem 6:} For the data set described in problem 3, build a LDA classifier. That is, train sample means for each class and population co-variance, and classify based on the linear discriminant functions in $\delta_k^l=\bm{x}^T\hat{R}^{-1}\hat{\bm{\mu}}_k - \frac{1}{2}\hat{\bm{\mu}}_k^T\hat{R}^{-1}\hat{\bm{\mu}}_k + \text{log}\pi_k$. Characterize the error rate on the training data and on 10,000 points of test data. Plot the classification regions as before.

        The graph seen in \reff{fig:lda} shows the effectiveness of the linear discriminant analysis. The error rates and results for the LDA can be seen in \reff{tab:compare}, and the code for the classifier is seen following \reff{fig:qda}. 

        \newFigure{./media/lda.pdf}{Linear Discriminant Analysis}{0.64}{fig:lda}

        \subsection{Problem 7:} For the data set described in problem 3, build a QDA classifier. In this case, you will also need to build the class co-variance matrices. Classify based on the quadratic discriminant functions in the equation $\text{log}\hat{\pi}_k -\frac{1}{2}\text{log}|\hat{R}_k|-\frac{1}{2}(\bm{x}-\bm{\hat{\mu}}_k)^T\hat{R}_k^{-1}(\bm{x}-\hat{\bm{\mu}}_k) $. Characterize the error rate on the training data and on 10,000 points of test data. Plot the classification regions as before. Compare the decision boundaries between QDA and quadratic regression.

        The plotted data for Problem 7 can be seen in \reff{fig:qda}. The co-variance matrix can be seen in the second code block below, and it is referenced to as \texttt{Rhat} in the code. The decision boundaries between the LDA and QDA are significantly different; the LDA has a linear shape almost exactly the same as the Linear Regression, whereas the QDA has a steep curve towards the \texttt{class1} data as seen in \reff{fig:qda}. This curve can allow the classifier to be more sensitive to nonlinearities.

        \newFigure{./media/qda.pdf}{Quadratic Discriminant Function Classifier}{0.65}{fig:qda}

        \lstinputlisting[language=Python]{../python/6_lda.py}\label{code:lda}

        \lstinputlisting[language=Python]{../python/7_qda.py}\label{code:qda}


    }

    \DeclareDocumentCommand{\linLogisticRegression}{} {
         \section{Linear Logistic Regression}
        \subsection{Problem 8:} Using the probability model $P(Y=0|X=\bm{x})=\frac{1}{1+\text{exp}[-\beta^T\bm{x}]} $ , show that $l(\beta)$ can be written as
        \begin{equation*}
            l(\beta)=\sum_{i=1}^N y_i \beta^T\bm{x}_i - log(1+e^{\beta^T\bm{x}_i})
        \end{equation*}

        The following note was necessary to remove a negative sign from the leading term of the result.

        Note:
        \begin{equation*}
            \frac{e^{\beta^T\bm{x}_i}}{1 + e^{\beta^T\bm{x}_i}} 
            = \frac{1}{1 + e^{-\beta^T\bm{x}_i}} 
        \end{equation*}


        We begin with the equation:
        \begin{equation*}\begin{aligned}
            l(\beta) &= \sum_{i=1}^N y_i \text{ log }p(\bm{x}_i;\beta)
            + (1 - y_i) \text{ log}(1-p(\bm{x}_i;\beta))\\
            &= \sum_{i=1}^N y_i \text{ log}( \frac{1}{1+e^{-\beta^T\bm{x}_i}} )
            + (1-y_i) \text{ log}(1 - \frac{1}{1+e^{-\beta^T\bm{x}_i}})\\
            &= \sum_{i=1}^N -y_i \text{ log}( 1+e^{-\beta^T\bm{x}_i})
            + (1-y_i) \text{ log}(\frac{1+e^{-\beta^T\bm{x}_i}}{1+e^{-\beta^T\bm{x}_i}} - \frac{1}{1+e^{-\beta^T\bm{x}_i}})\\
            &= \sum_{i=1}^N -y_i \text{ log}( 1+e^{-\beta^T\bm{x}_i})
            + (1-y_i) \text{ log}(\frac{e^{-\beta^T\bm{x}_i}}{1+e^{-\beta^T\bm{x}_i}})\\
            &= \sum_{i=1}^N -y_i \text{ log}( 1+e^{-\beta^T\bm{x}_i})
            + (1-y_i) (\text{log}(e^{-\beta^T\bm{x}_i})-\text{log}({1+e^{-\beta^T\bm{x}_i}}))\\
            &= \sum_{i=1}^N -y_i \text{ log}( 1 + e^{-\beta^T\bm{x}_i})
            + (1-y_i) (-\beta^T\bm{x}_i - \text{log}({1+e^{-\beta^T\bm{x}_i}}))\\
            &= \sum_{i=1}^N -y_i \text{ log}( 1 + e^{-\beta^T\bm{x}_i})
            -\beta^T\bm{x}_i - \text{log}({1+e^{-\beta^T\bm{x}_i}})
            + y_i\beta^T\bm{x}_i + y_i \text{ log}( 1 + e^{-\beta^T\bm{x}_i})\\
            &= \sum_{i=1}^N 
            y_i\beta^T\bm{x}_i
            -\beta^T\bm{x}_i - \text{log}({1+e^{-\beta^T\bm{x}_i}})\\
            &= \sum_{i=1}^N 
            y_i\beta^T\bm{x}_i
            -(\beta^T\bm{x}_i + \text{log}({1+e^{-\beta^T\bm{x}_i}}))\\
            &= \sum_{i=1}^N 
            y_i\beta^T\bm{x}_i
            -(\text{log}(e^{\beta^T\bm{x}_i}) + \text{log}({1+e^{-\beta^T\bm{x}_i}}))\\
        \end{aligned}\end{equation*}

        \begin{equation}
            l(\beta)  = \sum_{i=1}^N y_i\beta^T\bm{x}_i - \text{log}({1+e^{\beta^T\bm{x}_i}}))
        \end{equation}

        \subsection{Problem 9:} Show that 
        \begin{equation*}
            \frac{\partial l(\beta)}{\partial \beta} = \sum_{i=1}^N\bm{x}_i (y_i - p(\bm{x}_i;\beta) )
        \end{equation*}

        Starting with the equation from the previous problem.

        \begin{equation*}\begin{aligned}
            \frac{\partial}{\partial \beta} &\sum_{i=1}^N y_i\beta^T\bm{x}_i - \text{log}({1+e^{\beta^T\bm{x}_i}})\\
             &\sum_{i=1}^N y_i\bm{x}_i - \frac{\partial}{\partial \beta}\text{log}({1+e^{\beta^T\bm{x}_i}})\\
             &\sum_{i=1}^N y_i\bm{x}_i - \frac{\bm{x}_i e^{\beta^T\bm{x}_i}}{1+e^{\beta^T\bm{x}_i}}\\
             &\sum_{i=1}^N y_i\bm{x}_i - \frac{\bm{x}_i e^{\beta^T\bm{x}_i}}
             {1+e^{\beta^T\bm{x}_i}}\\
             &\sum_{i=1}^N y_i\bm{x}_i - \bm{x}_i p(\bm{x}_i;\beta)\\
        \end{aligned}\end{equation*}

        \begin{equation}
            \sum_{i=1}^N \bm{x}_i (y_i - p(\bm{x}_i;\beta) )
        \end{equation}

        \subsection{Problem 10:} Show that 
        \begin{equation*}
            \frac{\partial^2 l(\beta)}{\partial \beta \partial \beta^T} = \sum_{i=1}^N \bm{x}_i\bm{x}_i^T
            p(\bm{x}_i;\beta)(1-p(\bm{x}_i;\beta))
        \end{equation*}

        Beginning with the first derivative from the previous problem:
        \begin{equation*}\begin{aligned}
            \frac{\partial}{\partial \beta} &\sum_{i=1}^N 
            \bm{x}_i (y_i - p(\bm{x}_i;\beta))\\
            &\sum_{i=1}^N 
            -\frac{\partial}{\partial\beta} \bm{x}_i p(\bm{x}_i;\beta))\\
            &\sum_{i=1}^N 
            -\bm{x}_i \frac{\partial}{\partial\beta}
            \frac{1}{1+e^{\beta^T\bm{x}_i}} \\
            &\sum_{i=1}^N 
            \frac{-\bm{x}_i e^{\beta^T\bm{x}_i}x^T}
            {(1+e^{\beta^T\bm{x}_i})^2}\\
            &\sum_{i=1}^N 
            \frac{-\bm{x}_i e^{\beta^T\bm{x}_i}x^T}
            {(1+e^{\beta^T\bm{x}_i})^2}\\
        \end{aligned}\end{equation*}

        Note:
        \begin{equation*}\begin{aligned}
            1 - p(\bm{x}_i;\beta) 
            &= \frac{1 + e^{\beta^T\bm{x}_i}}{1 + e^{\beta^T\bm{x}_i}}
            - \frac{1}{1 + e^{\beta^T\bm{x}_i}}\\
            &= \frac{e^{\beta^T\bm{x}_i}}{1 + e^{\beta^T\bm{x}_i}}
        \end{aligned}\end{equation*}

        So,
       \begin{equation}\begin{aligned}
            \sum_{i=1}^N 
            \frac{-\bm{x}_i e^{\beta^T\bm{x}_i}x^T}
            {(1+e^{\beta^T\bm{x}_i})^2} &= 
            \frac{-\bm{x}_i^T\bm{x}(1 - p(\bm{x}_i;\beta))} 
            {1+e^{\beta^T\bm{x}_i}}\\
            &=-\bm{x}_i^T\bm{x}\: p(\bm{x}_i;\beta) (1 - p(\bm{x}_i;\beta))
       \end{aligned}\end{equation}

       \subsection{Problem 11:} Show that the Newton-Raphson update step can be written:

       \begin{equation*}\begin{aligned}
            \hat{\beta}^{[m+1]} &= (\bm{X}^T\bm{W}\bm{X})^{-1} 
            \bm{X}^T \bm{W}(\bm{X}\hat{\beta}^{[m]} + \bm{W}^{-1}(\bm{y}-\bm{p^{[m]}}))
       \end{aligned}\end{equation*}

       Using the definition of the Newton-Raphson update and the results from Problems 9-10:

       \begin{equation*}\begin{aligned}
           \hat{\beta}^{[m+1]} &= \hat{\beta}^{[m]} + (\bm{X}^T\bm{W}\bm{X})^{-1}
            \bm{X}^T(\bm{y}-\bm{p^{[m]}})\\
            &= (\bm{X}^T\bm{W}\bm{X})^{-1} \bm{X}^T \bm{W}\bm{X} \:\, \hat{\beta}^{[m]} 
            + (\bm{X}^T\bm{W}\bm{X})^{-1} 
            \bm{X}^T \: \bm{W} \bm{W}^{-1} \: (\bm{y}-\bm{p^{[m]}})
       \end{aligned}\end{equation*}

       \begin{equation}
            \hat{\beta}^{[m+1]} = (\bm{X}^T\bm{W}\bm{X})^{-1} 
            \bm{X}^T \bm{W}(\bm{X}\hat{\beta}^{[m]} + \bm{W}^{-1}(\bm{y}-\bm{p^{[m]}}))
       \end{equation}

        \subsection{Problem 12:} For the data set described in problem 3, program the logistic regression classifier. That is, program the IRLS algorithm to determine $\beta$ from the training data, then use it to compute the log-likelihood ratio. Use this for classification of the training data and 10,000 points of test data. Plot the classification regions as before. Record the probability of classification error for test and training data on the table.  

        $\beta$ was determined using the IRLS algorithm that was optimized to eliminate the need for the $\bm{W}$ matrix because it is all zeros except for the trace. The algorithm that I used can be found at \href{https://onlinecourses.science.psu.edu/stat857/node/64}{\underline{this link}}, or it can be seen in my code below. It takes about 10 iterations of $\beta$ to have an accurate classifier. The classification regions can be seen in \reff{fig:log}, and the probability errors for the test and training data are in \reff{tab:compare}.


        \newFigure{./media/log.pdf}{Logistic Classifier}{0.65}{fig:log}.

        \lstinputlisting[language=Python]{../python/12_log_regression.py}
        
    }

    \DeclareDocumentCommand{\kNearest}{} {
        \section{k-nearest Neighbor Classifier}
        \subsection{Problem 13:} For the data set described in problem 3, program a k-nearest neighbor function. Make it so that you can change the value of k. Use your k-nearest neighbor function for classification of the training data and 10,000 points of test data for k = 1, k = 5, and k = 15. Comment on the probability of error on the training data when k = 1. Plot the classification regions. Record the probability of classification error for test and training data on the table.

        As seen in the following code, the $k$ value can be easily manipulated to give the corresponding results seen in \reff{fig:kNearOneAndFive} through \reff{fig:k15}. 

        Table \ref{tab:nearest} shows how the nearest neighbor program reacts when k=1. The program overfits to the training data and reports ``0 Errors," but later has the highest error rate on the test data. 

        The classification regions can be seen in \reff{fig:kNearOneAndFive} and \reff{fig:k15}.


        \begin{table}[H]
            \begin{tabulary}{\linewidth}{rccc}	
                && \multicolumn{2}{c}                   {Errors in \%}      \\
                \toprule % --------------------------------------------------
                        Method	            & Run-time &    Training  &  Test     \\ 
                \toprule % ---------------------------------------------------
                1-Nearest Neighbor	    	    &  35.02s  &      00.0    &  21.83    \\
                5-Nearest Neighbor	    	    &  37.92s  &      12.0    &  20.29    \\
                15-Nearest Neighbor             &  36.47s  &      16.0    &  19.25    \\
                \bottomrule % ------------------------------------------------
            \end{tabulary}\caption{Nearest Neighbor Performance Comparison}\label{tab:nearest}\end{table} 


            \begin{figure}[H]
                \includegraphics[width=.45\linewidth]{./media/1-nearest.pdf}\hfill 
                \includegraphics[width=.45\linewidth]{./media/5-nearest.pdf}
                \caption[k=1 and k=5 Nearest Neighbor]{Left: k=1 Nearest Neighbor. Right: k=5 Nearest Neighbor}
                \label{fig:kNearOneAndFive}
            \end{figure}

            \newFigure{./media/15-nearest.pdf}{k=15 Nearest Neighbor}{0.45}{fig:k15}

            \lstinputlisting[language=Python]{../python/13_neighbor.py}

        }

            \DeclareDocumentCommand{\naiveBayes}{} {
                \section{Naive Bayes Classifier}
            \subsection{Problem 14:} The true density of the data is a Gaussian mixture represented by $f_X(x) = p_0 N(x, \mu_0,\sigma_0^2) + p_1 N(x, \mu_1 , \sigma_1^2)$. Using this equation, with given values of $\mu$ and $sigma$, make a plot of the true density, the sample empirical distribution, the kernel functions, and the estimated density. Try different values of $\lambda$.

            As seen in \reff{fig:parzen}, the Parzen distribution was calculated using an empirical distribution and adding the corresponding Gaussian distributions. When $\lambda=0.8$ was chosen, the best results can be seen. If $\lambda$ is too small, the data begins to look corrupt, and if it was much larger than $\lambda=0.8$, it became very elongated as seen in \reff{fig:bayes_lambda}.


            \newFigure{./media/parzen.pdf}{Calculating Parzen Distribution}{0.65}{fig:parzen}

            \begin{figure}[H]
                \includegraphics[width=.45\linewidth]{./media/bayes-large-lambda.pdf}\hfill 
                \includegraphics[width=.45\linewidth]{./media/bayes-small-lambda.pdf}
                \caption[Altering the $\lambda$ values in Parzen Distribution]{Left: $\lambda$ = 2. Right: $\lambda$ = 0.5}
                \label{fig:bayes_lambda}
            \end{figure}
            \lstinputlisting[language=Python]{../python/14_naive_bayes.py}


            \subsection{Problem 15:} Using the training data in \texttt{classassgntrain1.dat} to estimate the densities, apply the Naive Bayes estimator to our data set. Plot the classification regions. Record the probability of classification error for test and training data on the table.


            The data with the classification regions are plotted in \reff{fig:naive} and the code is typed out below the graph.
            The probability of classification error for test and training data are recorded on \reff{tab:compare}.

            To estimate the density functions, I simply found the probability that an x value would be chosen in \texttt{class0} and a y value in \texttt{class0}. Their pdf were calculated using the Parzen technique, and their distribution can be seen in \reff{fig:bayes1d}.

            Because the Bayes method assumes the probability is independent, the two can simply be multiplied together to create a probability that a given point is in class 0 or 1. \reff{fig:naive} shows how the division line ended up, and the classification error can be seen in \reff{tab:compare}.
            \newFigure{./media/bayes-1d-plot.pdf}{Bayes 1D plot of X and Y}{0.55}{fig:bayes1d}

            \newFigure{./media/naive-bayes.pdf}{Naive Bayes Parzen Graph}{0.65}{fig:naive}
            \lstinputlisting[language=Python]{../python/15_naive_bayes_applied.py}
        }

            \DeclareDocumentCommand{\optimalBayes}{} {
                \section{Optimal Bayes Classifier}
            \subsection{Problem 16:} For the data set described in problem 3, determine the Bayes error rate on the training data and 10,000 points of test data. Record the probability of classification error for test and training data on the table. Plot the classification regions.

            The classification regions for the Optimal Bayes Classifier can be seen in \reff{fig:optimal}. It performed the best on the test data above all other data sets because it had the actual data model embedded into the classifier. See \reff{tab:compare} for specific details.

            \newFigure{./media/optimal-bayes.pdf}{Optimal Bayes Classifier}{0.65}{fig:optimal}

        \lstinputlisting[language=Python]{../python/16_optimal_bayes.py}
    }

    \DeclareDocumentCommand{\discussion}{} {
        \section{Discussion}\label{sec:discussion}
        \subsection{Problem 17:} Discuss the relative merits of the different classification algorithms on this source of data. Comment on differences in performance between training and test data. Also, comment on operating speed of the classification algorithms (after they have been trained). Summarize what you have learned.  Turn in with this assignment your table of results, plots of data and classification regions, and listings of your \texttt{PYTHON} code.

        The highest performing data classifier was the Bayes Optimal classifier. It consistently performed with fewer errors when I increased the sample size of data to 10000 data points for each class. In real world applications, however, it is often not feasible to have such a precise model.

        The Linear regression model was similar in run-time to the optimal classifier, and it performed well, regardless of its seeming simplicity.

        The 15-nearest neighbor performed almost as well as the Bayes Optimal classifier, but it took considerably longer. There are ways to optimize the code, but the method requires the program to store all of the training data and process it continually--an obvious drawback. As seen in \reff{tab:compare}, most of the programs were within about a 20 percent error rate with the exception of the 1-Nearest Neighbor. They it had a 0 percent error on the training data, it was too ``fitted" to the original data to yield correct results.

        \begin{table}[H]
            \begin{tabulary}{\linewidth}{rccc}	
                && \multicolumn{2}{c}                   {Errors in \%}      \\
                \toprule % --------------------------------------------------
                        Method	            & Run-time &    Training  &  Test     \\ 
                \toprule % ---------------------------------------------------
                Linear Regression 		        &  1.23s   &      14.5    &  20.49    \\
                Quadratic Regression	        &  1.70s   &      14.5    &  20.44    \\
                Linear Discriminant Analysis    &  2.49s   &      15.0    &  19.98    \\
                Quadratic Discriminant Analysis	&  3.26s   &      14.5    &  20.23    \\
                Logistic Regression     	    &  2.00s   &      14.0    &  20.00    \\
                1-Nearest Neighbor	    	    &  35.02s  &      00.0    &  21.83    \\
                5-Nearest Neighbor	    	    &  37.92s  &      12.0    &  20.29    \\
                15-Nearest Neighbor             &  36.47s  &      16.0    &  19.25    \\
                Bayes Naive 	                &  1.22s   &      14.0    &  20.04    \\
                Bayes Optimal Classifier        &  0.20s   &      14.0    &  19.14    \\
                \bottomrule % ------------------------------------------------
            \end{tabulary}\caption{Binary Classifier Performance Comparison}\label{tab:compare}  \end{table} 

        }
            \begin{document}
            \main
            \end{document}
